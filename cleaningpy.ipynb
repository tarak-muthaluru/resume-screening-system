{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5509a64e-bced-4043-b0ff-a43bfe0f5aac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thara\\resume-screen\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c162196-0e3b-4926-9bf9-1aedb23a54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Test User\n",
      "Experience: 2 years\n",
      "Skills: Python, ML\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open(\"sample_data/resume1.txt\", \"r\", encoding=\"utf-16\", errors=\"ignore\") as f:\n",
    "    text = f.read()\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a48b593e-28d1-40ca-9950-959399a4e938",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Test User Experience: 2 years Skills: Python, ML\n"
     ]
    }
   ],
   "source": [
    "from cleaning import clean_text\n",
    "\n",
    "cleaned = clean_text(text)\n",
    "print(cleaned)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ba983fec-d16f-46de-afd5-9081cdfae834",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of pages: 1\n",
      "\n",
      "--- First 1000 characters ---\n",
      "\n",
      "MUTHALURU THARAK NARASIMHA\n",
      "Data Analyst Intern | Aspiring Data Scientist\n",
      "Phone: +91 86396 97673 Email: tharakm21@gmail.com\n",
      "Linked IN: linkedin.com/in/muthaluru-tarak-b2301a285 Git hub: https://github.com/tarak-muthaluru\n",
      "PROFESSIONAL SUMMARY:\n",
      "Detail-oriented Data Science trainee and Data Analyst Intern with practical experience in Python, SQL, and end-to-end machine\n",
      "learning workflows. Skilled in data cleaning, feature engineering, model development, and performance evaluation using Scikit-\n",
      "learn, Pandas, and NumPy. Experienced in creating dashboards and analytical reports with Power BI, Excel, and visualization\n",
      "libraries. Familiar with AI concepts, prompt engineering, and building basic generative AI agents for automation. Strong\n",
      "problem-solving abilities and a solid foundation in data-driven decision-making.\n",
      "TECHNICAL SKILLS:\n",
      "• Programming: Python, SQL\n",
      "• Data Analytics & Workflow Optimization: Analytical Skills, Workflow Analytics, ETL & Data Pipeline Creation, Data\n",
      "Workflow Optimization, Power Query Automation\n",
      "• Automation Tools: Python Automation (data cleaning, processing), Power Query, Excel Automation\n",
      "• Machine Learning & AI: Supervised & Unsupervised ML, Feature Engineering, Model Evaluation, Scikit-learn, NumPy,\n",
      "Pandas, Hyperparameter Tuning, ML Pipelines, Prompt Engineering, LLM Workflow Design, Basic AI Agent Development\n",
      "• Data Visualization: Power BI, Matplotlib, Seaborn\n",
      "• Databases: MySQL, PostgreSQL\n",
      "• Tools & Platforms: Jupyter Notebook, GitHub, MS Excel, Power Query, VS Code\n",
      "EXPERIENCE:\n",
      "• Data Analyst Intern – Abhyaz (MTAB Technologies Pvt Ltd)\n",
      "May 2025 – Sep 2025\n",
      "o Collected and imported business data into Zoho CRM and maintained clean data pipelines.\n",
      "o Created interactive Power BI dashboards and performed EDA to generate actionable insights.\n",
      "o Performed SQL-based data extraction and automated repetitive cleaning tasks using Python, improving data processing\n",
      "speed.\n",
      "KEY PROJECTS:\n",
      "• Movie Recommendation Engine | Python Development:\n",
      "o Built a content-based movie recommendation engine using TF-IDF vectorization and cosine similarity.\n",
      "o Processed 8,800+ Netflix titles to generate top 5 real-time movie recommendations.\n",
      "o Tech: Python, Pandas, NumPy, Scikit-learn, NLTK\n",
      "o GitHub: https://github.com/tarak-muthaluru/Netflix-Dataset-Exploration-Trends-Genres-and-Content-Insights\n",
      "• Retail Sales Performance Dashboard | Power BI Project:\n",
      "o Built an interactive Power BI dashboard for Superstore data with KPIs, regional trends, profit insights, and customer\n",
      "segmentation using DAX and drill-down analytics.\n",
      "o Improved reporting efficiency by 70% through automated visuals, slicers, and dynamic metric calculations.\n",
      "o Tools: Power BI, DAX, Power Query\n",
      "o GitHub: https://github.com/tarak-muthaluru/Retail-Sales-Performance-Customer-Insights-Dashboard\n",
      "EDUCATION:\n",
      "• Bachelor of Technology (ECE) • Intermediate (MPC)\n",
      "Nehru Institute of Engineering and Technology, Narayana Junior College, Nellore, Andhra Pradesh\n",
      "Coimbatore (2020–2024) (2018–2020)\n",
      "CGPA: 7.8/10 CGPA: 9.3/10\n",
      "CERTIFICATIONS:\n",
      "• Data Science – CedLearn Institute\n",
      "• Introduction to Data Science, Machine Learning & Data Visualization – Infosys Springboard\n",
      "• Python Essentials 1 & 2 – Cisco Networking Academy\n",
      "Languages: English, Telugu | Availability: Immediate | Open to Relocate\n"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "pdf_path = \"sample_data/resume_pdf1.pdf.pdf\"\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    print(\"Number of pages:\", len(pdf.pages))\n",
    "    first_page = pdf.pages[0]\n",
    "    text = first_page.extract_text()\n",
    "\n",
    "print(\"\\n--- First 1000 characters ---\\n\")\n",
    "print(text if text else text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c976862f-c709-4729-b715-f060c55b0a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MUTHALURU THARAK NARASIMHA Data Analyst Intern | Aspiring Data Scientist Phone: +91 86396 97673 Email: tharakm21@gmail.com Linked IN: linkedin.com/in/muthaluru-tarak-b2301a285 Git hub: https://github.com/tarak-muthaluru PROFESSIONAL SUMMARY: Detail-oriented Data Science trainee and Data Analyst Intern with practical experience in Python, SQL, and end-to-end machine learning workflows. Skilled in data cleaning, feature engineering, model development, and performance evaluation using Scikit- learn, Pandas, and NumPy. Experienced in creating dashboards and analytical reports with Power BI, Excel, and visualization libraries. Familiar with AI concepts, prompt engineering, and building basic generative AI agents for automation. Strong problem-solving abilities and a solid foundation in data-driven decision-making. TECHNICAL SKILLS: • Programming: Python, SQL • Data Analytics & Workflow Optimization: Analytical Skills, Workflow Analytics, ETL & Data Pipeline Creation, Data Workflow Optimization, Power Query Automation • Automation Tools: Python Automation (data cleaning, processing), Power Query, Excel Automation • Machine Learning & AI: Supervised & Unsupervised ML, Feature Engineering, Model Evaluation, Scikit-learn, NumPy, Pandas, Hyperparameter Tuning, ML Pipelines, Prompt Engineering, LLM Workflow Design, Basic AI Agent Development • Data Visualization: Power BI, Matplotlib, Seaborn • Databases: MySQL, PostgreSQL • Tools & Platforms: Jupyter Notebook, GitHub, MS Excel, Power Query, VS Code EXPERIENCE: • Data Analyst Intern – Abhyaz (MTAB Technologies Pvt Ltd) May 2025 – Sep 2025 o Collected and imported business data into Zoho CRM and maintained clean data pipelines. o Created interactive Power BI dashboards and performed EDA to generate actionable insights. o Performed SQL-based data extraction and automated repetitive cleaning tasks using Python, improving data processing speed. KEY PROJECTS: • Movie Recommendation Engine | Python Development: o Built a content-based movie recommendation engine using TF-IDF vectorization and cosine similarity. o Processed 8,800+ Netflix titles to generate top 5 real-time movie recommendations. o Tech: Python, Pandas, NumPy, Scikit-learn, NLTK o GitHub: https://github.com/tarak-muthaluru/Netflix-Dataset-Exploration-Trends-Genres-and-Content-Insights • Retail Sales Performance Dashboard | Power BI Project: o Built an interactive Power BI dashboard for Superstore data with KPIs, regional trends, profit insights, and customer segmentation using DAX and drill-down analytics. o Improved reporting efficiency by 70% through automated visuals, slicers, and dynamic metric calculations. o Tools: Power BI, DAX, Power Query o GitHub: https://github.com/tarak-muthaluru/Retail-Sales-Performance-Customer-Insights-Dashboard EDUCATION: • Bachelor of Technology (ECE) • Intermediate (MPC) Nehru Institute of Engineering and Technology, Narayana Junior College, Nellore, Andhra Pradesh Coimbatore (2020–2024) (2018–2020) CGPA: 7.8/10 CGPA: 9.3/10 CERTIFICATIONS: • Data Science – CedLearn Institute • Introduction to Data Science, Machine Learning & Data Visualization – Infosys Springboard • Python Essentials 1 & 2 – Cisco Networking Academy Languages: English, Telugu | Availability: Immediate | Open to Relocate\n"
     ]
    }
   ],
   "source": [
    "from cleaning import clean_text\n",
    "\n",
    "cleaned_pdf_text = clean_text(text)\n",
    "\n",
    "print(cleaned_pdf_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d0aacabd-8b81-49e0-92a1-7313dc0efcbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pytesseract OK\n",
      "pdf2image OK\n",
      "Pillow OK\n"
     ]
    }
   ],
   "source": [
    "import pytesseract\n",
    "import pdf2image\n",
    "from PIL import Image\n",
    "\n",
    "print(\"pytesseract OK\")\n",
    "print(\"pdf2image OK\")\n",
    "print(\"Pillow OK\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb4945f7-9c41-493b-a88d-b3881cf67050",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pages converted to images: 1\n",
      "\n",
      "--- OCR first 800 characters ---\n",
      "\n",
      "LIAM ANDERSON\n",
      "\n",
      "Scanner Operator\n",
      "© support@qwikresumecom \\ (123)4567899 9 Los Angeles\n",
      "\n",
      "@ wwwaqwikresume.com\n",
      "\n",
      "Pe] PROFESSIONAL SUMMARY\n",
      "\n",
      "With 2 years of experience as a Scanner Operator, | excel in high-\n",
      "speed document imaging and data verification. My expertise in\n",
      "operating advanced scanning equipment ensures meticulous quality\n",
      "control and efficient handling of sensitive information. | am dedicated\n",
      "to enhancing scanning processes and contributing positively to team\n",
      "objectives in dynamic environments.\n",
      "\n",
      "@ work EXPERIENCE\n",
      "\n",
      "Scanner Operator £8 Feb / 2024-Ongoing\n",
      "Quantum Solutions LLC % Phoenix, AZ\n",
      "\n",
      "1. Executed high-speed scanning of documents ensuring accuracy\n",
      "and quality control\n",
      "\n",
      "2. Verified and organized scanned materials for new and used car\n",
      "sales processing.\n",
      "\n",
      "3. Maintained meticulous records\n"
     ]
    }
   ],
   "source": [
    "from pdf2image import convert_from_path\n",
    "import pytesseract\n",
    "\n",
    "pdf_path = r\"C:\\Users\\thara\\resume-screen\\sample_data\\scanned_resume.pdf.pdf\"\n",
    "\n",
    "# 1) Convert first page of PDF to image\n",
    "images = convert_from_path(pdf_path, dpi=200)\n",
    "print(\"Pages converted to images:\", len(images))\n",
    "\n",
    "# 2) Run OCR on first page\n",
    "first_image = images[0]\n",
    "ocr_text = pytesseract.image_to_string(first_image)\n",
    "\n",
    "print(\"\\n--- OCR first 800 characters ---\\n\")\n",
    "print(ocr_text[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "503030c7-ec51-4827-992a-107bd3967c0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIAM ANDERSON Scanner Operator © support@qwikresumecom \\ (123)4567899 9 Los Angeles @ wwwaqwikresume.com Pe] PROFESSIONAL SUMMARY With 2 years of experience as a Scanner Operator, | excel in high- speed document imaging and data verification. My expertise in operating advanced scanning equipment ensures meticulous quality control and efficient handling of sensitive information. | am dedicated to enhancing scanning processes and contributing positively to team objectives in dynamic environments. @ work EXPERIENCE Scanner Operator £8 Feb / 2024-Ongoing Quantum Solutions LLC % Phoenix, AZ 1. Executed high-speed scanning of documents ensuring accuracy and quality control 2. Verified and organized scanned materials for new and used car sales processing. 3. Maintained meticulous records of scann\n"
     ]
    }
   ],
   "source": [
    "from cleaning import clean_text   # this should work now since cleaning module is working\n",
    "\n",
    "cleaned_ocr_text = clean_text(ocr_text)\n",
    "print(cleaned_ocr_text[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e677d71-0ea2-47b6-9848-0eb1611e8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Path', '__builtins__', '__cached__', '__doc__', '__file__', '__loader__', '__name__', '__package__', '__spec__', 'clean_text', 'convert_from_path', 'extract_pdf_text', 'pdfplumber', 'pytesseract']\n"
     ]
    }
   ],
   "source": [
    "import file_io\n",
    "print(dir(file_io))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29f27b42-01d0-486b-8848-1f506896f117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "import successful\n"
     ]
    }
   ],
   "source": [
    "from file_io import extract_pdf_text\n",
    "print(\"import successful\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c72097ab-0802-44d2-baca-c57ebbf9215c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LIAM ANDERSON Scanner Operator © support@qwikresumecom \\ (123)4567899 9 Los Angeles @ wwwaqwikresume.com Pe] PROFESSIONAL SUMMARY With 2 years of experience as a Scanner Operator, | excel in high- speed document imaging and data verification. My expertise in operating advanced scanning equipment ensures meticulous quality control and efficient handling of sensitive information. | am dedicated to enhancing scanning processes and contributing positively to team objectives in dynamic environments. @ work EXPERIENCE Scanner Operator £8 Feb / 2024-Ongoing Quantum Solutions LLC % Phoenix, AZ 1. Executed high-speed scanning of documents ensuring accuracy and quality control 2. Verified and organized scanned materials for new and used car sales processing. 3. Maintained meticulous records of scann\n"
     ]
    }
   ],
   "source": [
    "from file_io import extract_pdf_text\n",
    "\n",
    "pdf_path = \"sample_data/scanned_resume.pdf.pdf\"   # or resume_pdf1.pdf\n",
    "cleaned_text = extract_pdf_text(pdf_path)\n",
    "\n",
    "print(cleaned_text[:800])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "33103fca-0166-4e14-9ffe-80901b34719c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\thara\\\\resume-screen'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "49236941-6c7f-475d-a24c-f14ad43d54ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c029447eae9d40d8855a02239df36aaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\thara\\resume-screen\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\thara\\.cache\\huggingface\\hub\\models--sentence-transformers--all-MiniLM-L6-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "201f1c4a888c489da217a2da1d3b28bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "175a1470b9fb4bb4804ae4e3afb68361",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6145f89a54934081b8baa6ddcc51d59d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "af69bab3f336409f9c44900f6d5f27ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d487ae7553144cf0b344b37631fc232f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "857b9088b74c4dfc9562a4bfdf6dd408",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "77e23566c3484a6990c572e0384eda40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9660c82e21e4f72afd03f8e6748d220",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "326dc6b298094554929f6ae902771f4e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdeb2c0c7df1418d81d0b18c5b842385",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded.\n",
      "\n",
      "Sample text:\n",
      "I am a data scientist with experience in Python and machine learning.\n",
      "\n",
      "Type of embedding: <class 'numpy.ndarray'>\n",
      "Embedding shape: (384,)\n",
      "\n",
      "First 5 values of embedding: [-0.07049254 -0.03874239  0.01783427  0.08103763  0.01738799]\n",
      "Sample text:\n",
      "I am a data scientist with experience in Python and machine learning.\n",
      "\n",
      "Type of embedding: <class 'numpy.ndarray'>\n",
      "Embedding shape attribute: (384,)\n",
      "Length: 384\n",
      "\n",
      "First 5 values: [-0.07049254 -0.03874239  0.01783427  0.08103763  0.01738799]\n"
     ]
    }
   ],
   "source": [
    "from embedding_test import get_embedding\n",
    "\n",
    "sample_text = \"I am a data scientist with experience in Python and machine learning.\"\n",
    "emb = get_embedding(sample_text)\n",
    "\n",
    "print(\"Sample text:\")\n",
    "print(sample_text)\n",
    "\n",
    "print(\"\\nType of embedding:\", type(emb))\n",
    "\n",
    "shape = getattr(emb, \"shape\", None)\n",
    "print(\"Embedding shape attribute:\", shape)\n",
    "\n",
    "try:\n",
    "    print(\"Length:\", len(emb))\n",
    "except TypeError:\n",
    "    print(\"Length: not available\")\n",
    "\n",
    "try:\n",
    "    print(\"\\nFirst 5 values:\", emb[:5])\n",
    "except Exception as e:\n",
    "    print(\"\\nCould not slice embedding:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "83fb8419-a68e-4c95-a4f5-bc9576dd0636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD text (first 400 chars):\n",
      "resume-screen/jd.txt\n",
      "\n",
      "Resume text (first 400 chars):\n",
      "MUTHALURU THARAK NARASIMHA Data Analyst Intern | Aspiring Data Scientist Phone: +91 86396 97673 Email: tharakm21@gmail.com Linked IN: linkedin.com/in/muthaluru-tarak-b2301a285 Git hub: https://github.com/tarak-muthaluru PROFESSIONAL SUMMARY: Detail-oriented Data Science trainee and Data Analyst Intern with practical experience in Python, SQL, and end-to-end machine learning workflows. Skilled in d\n",
      "\n",
      "Similarity score between this JD and your resume: 0.11883026361465454\n"
     ]
    }
   ],
   "source": [
    "from file_io import extract_pdf_text\n",
    "from embedding import get_embedding, similarity\n",
    "\n",
    "# 1) Get resume text from your existing PDF\n",
    "resume_path = \"sample_data/resume_pdf1.pdf.pdf\"  # use your actual resume file path\n",
    "resume_text = extract_pdf_text(resume_path)\n",
    "\n",
    "# 2) Manually define a JD as a Python string (no file needed)\n",
    "jd_text = \"resume-screen/jd.txt\"\n",
    "\n",
    "# 3) Print some parts of both to check\n",
    "print(\"JD text (first 400 chars):\")\n",
    "print(jd_text[:400])\n",
    "\n",
    "print(\"\\nResume text (first 400 chars):\")\n",
    "print(resume_text[:400])\n",
    "\n",
    "# 4) Get embeddings\n",
    "jd_emb = get_embedding(jd_text)\n",
    "resume_emb = get_embedding(resume_text)\n",
    "\n",
    "# 5) Compute similarity\n",
    "score = similarity(jd_emb, resume_emb)\n",
    "\n",
    "print(\"\\nSimilarity score between this JD and your resume:\", score)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9bca59bd-9973-4f35-815a-85b477dc7a8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ranking of resumes for this JD:\n",
      "\n",
      "1. Resume A - Strong Data Scientist  --> similarity score = 0.6863\n",
      "2. Resume B - Somewhat Related (Data Analyst)  --> similarity score = 0.6173\n",
      "3. Resume C - Not Related (Sales)  --> similarity score = 0.2949\n"
     ]
    }
   ],
   "source": [
    "from embedding import get_embedding, similarity\n",
    "\n",
    "# 1) Define one JD (job description) as a string\n",
    "jd_text = \"\"\"\n",
    "We are looking for a data scientist with strong skills in Python, machine learning,\n",
    "data analysis, and experience building classification and regression models.\n",
    "The candidate should know Pandas, NumPy, statistics, and SQL.\n",
    "\"\"\"\n",
    "\n",
    "# 2) Define three example \"resumes\" as strings\n",
    "resume_texts = [\n",
    "    {\n",
    "        \"name\": \"Resume A - Strong Data Scientist\",\n",
    "        \"text\": \"\"\"\n",
    "        I am a data scientist with 2 years of experience in Python and machine learning.\n",
    "        I have built multiple classification and regression models using Pandas, NumPy,\n",
    "        scikit-learn and I regularly work with SQL for data analysis.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Resume B - Somewhat Related (Data Analyst)\",\n",
    "        \"text\": \"\"\"\n",
    "        I work as a data analyst using Excel, Power BI, and some Python.\n",
    "        I have basic experience in statistics and reporting, and I am learning\n",
    "        machine learning models. I mostly focus on dashboards and business reports.\n",
    "        \"\"\"\n",
    "    },\n",
    "    {\n",
    "        \"name\": \"Resume C - Not Related (Sales)\",\n",
    "        \"text\": \"\"\"\n",
    "        I am a sales executive with experience in client relationships,\n",
    "        marketing campaigns, lead generation, and negotiating deals.\n",
    "        I work with CRM tools and manage a small sales team.\n",
    "        \"\"\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# 3) Compute embedding for the JD once\n",
    "jd_emb = get_embedding(jd_text)\n",
    "\n",
    "# 4) For each resume, compute similarity with the JD\n",
    "results = []\n",
    "for item in resume_texts:\n",
    "    name = item[\"name\"]\n",
    "    text = item[\"text\"]\n",
    "    emb = get_embedding(text)\n",
    "    score = similarity(jd_emb, emb)\n",
    "    results.append((name, score))\n",
    "\n",
    "# 5) Sort results by similarity score (highest first)\n",
    "results_sorted = sorted(results, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# 6) Print ranking\n",
    "print(\"Ranking of resumes for this JD:\\n\")\n",
    "for rank, (name, score) in enumerate(results_sorted, start=1):\n",
    "    print(f\"{rank}. {name}  --> similarity score = {score:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "99691cb7-603b-4320-ab87-25a173f5f012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JD text (first 300 chars):\n",
      "\n",
      "We are looking for a data scientist with strong skills in Python, machine learning,\n",
      "data analysis, and experience building classification and regression models.\n",
      "The candidate should know Pandas, NumPy, statistics, and SQL.\n",
      "\n",
      "\n",
      "Resume text (first 300 chars):\n",
      "MUTHALURU THARAK NARASIMHA Data Analyst Intern | Aspiring Data Scientist Phone: +91 86396 97673 Email: tharakm21@gmail.com Linked IN: linkedin.com/in/muthaluru-tarak-b2301a285 Git hub: https://github.com/tarak-muthaluru PROFESSIONAL SUMMARY: Detail-oriented Data Science trainee and Data Analyst Inte\n",
      "\n",
      "Raw similarity score: 0.5401497483253479\n",
      "Human-readable: 54.0% match – Weak match. Only partially aligned. May not be the best fit.\n"
     ]
    }
   ],
   "source": [
    "from embedding import get_embedding, similarity\n",
    "from file_io import extract_pdf_text\n",
    "\n",
    "# 1) Get resume text from PDF (same as before)\n",
    "resume_path = \"sample_data/resume_pdf1.pdf.pdf\"  # your real resume file\n",
    "resume_text = extract_pdf_text(resume_path)\n",
    "\n",
    "# 2) Define a JD as a string (you can modify this text anytime)\n",
    "jd_text = \"\"\"\n",
    "We are looking for a data scientist with strong skills in Python, machine learning,\n",
    "data analysis, and experience building classification and regression models.\n",
    "The candidate should know Pandas, NumPy, statistics, and SQL.\n",
    "\"\"\"\n",
    "\n",
    "# 3) Helper: convert score into human-readable explanation\n",
    "def explain_score(score: float) -> str:\n",
    "    \"\"\"\n",
    "    Convert similarity score (0 to 1) into human-readable text.\n",
    "    \"\"\"\n",
    "    percentage = score * 100\n",
    "\n",
    "    if score >= 0.75:\n",
    "        level = \"Excellent match\"\n",
    "        note = \"This resume is highly aligned with the JD.\"\n",
    "    elif score >= 0.55:\n",
    "        level = \"Good match\"\n",
    "        note = \"Many skills are aligned, but there may be some gaps.\"\n",
    "    elif score >= 0.35:\n",
    "        level = \"Weak match\"\n",
    "        note = \"Only partially aligned. May not be the best fit.\"\n",
    "    else:\n",
    "        level = \"Poor match\"\n",
    "        note = \"Resume is mostly unrelated to the JD.\"\n",
    "\n",
    "    return f\"{percentage:.1f}% match – {level}. {note}\"\n",
    "\n",
    "# 4) Compute embeddings and similarity\n",
    "jd_emb = get_embedding(jd_text)\n",
    "resume_emb = get_embedding(resume_text)\n",
    "score = similarity(jd_emb, resume_emb)\n",
    "\n",
    "# 5) Print human-readable result\n",
    "print(\"JD text (first 300 chars):\")\n",
    "print(jd_text[:300])\n",
    "\n",
    "print(\"\\nResume text (first 300 chars):\")\n",
    "print(resume_text[:300])\n",
    "\n",
    "print(\"\\nRaw similarity score:\", score)\n",
    "print(\"Human-readable:\", explain_score(score))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d818296a-2e30-476a-abd2-8f2aa5325831",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full resume text (first 500 chars):\n",
      "LIAM ANDERSON Scanner Operator © support@qwikresumecom \\ (123)4567899 9 Los Angeles @ wwwaqwikresume.com Pe] PROFESSIONAL SUMMARY With 2 years of experience as a Scanner Operator, | excel in high- speed document imaging and data verification. My expertise in operating advanced scanning equipment ensures meticulous quality control and efficient handling of sensitive information. | am dedicated to enhancing scanning processes and contributing positively to team objectives in dynamic environments. \n",
      "\n",
      "=== SKILLS SECTION (first 500 chars) ===\n",
      "to recall department codes for efficient package sorting. @\n",
      "\n",
      "=== EXPERIENCE SECTION (first 500 chars) ===\n",
      "as a Scanner Operator, | excel in high- speed document imaging and data verification. My expertise in operating advanced scanning equipment ensures meticulous quality control and efficient handling of sensitive information. | am dedicated to enhancing scanning processes and contributing positively to team objectives in dynamic environments. @ work EXPERIENCE Scanner Operator £8 Feb / 2024-Ongoing Quantum Solutions LLC % Phoenix, AZ 1. Executed high-speed scanning of documents ensuring accuracy a\n"
     ]
    }
   ],
   "source": [
    "from file_io import extract_pdf_text\n",
    "from resume_sections import extract_resume_sections\n",
    "\n",
    "# 1) Load your resume text from PDF\n",
    "resume_path = \"sample_data/scanned_resume.pdf.pdf\"  # your existing file\n",
    "full_text = extract_pdf_text(resume_path)\n",
    "\n",
    "print(\"Full resume text (first 500 chars):\")\n",
    "print(full_text[:500])\n",
    "\n",
    "# 2) Extract sections\n",
    "sections = extract_resume_sections(full_text)\n",
    "\n",
    "print(\"\\n=== SKILLS SECTION (first 500 chars) ===\")\n",
    "print(sections[\"skills\"][:500])\n",
    "\n",
    "print(\"\\n=== EXPERIENCE SECTION (first 500 chars) ===\")\n",
    "print(sections[\"experience\"][:500])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fabe4a-3739-4ba6-98ce-30b747b41dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model...\n",
      "Model loaded.\n",
      "=== RESUME - SKILLS SECTION (first 300 chars) ===\n",
      "to recall department codes for efficient package sorting. @\n",
      "\n",
      "=== RESUME - EXPERIENCE SECTION (first 300 chars) ===\n",
      "as a Scanner Operator, | excel in high- speed document imaging and data verification. My expertise in operating advanced scanning equipment ensures meticulous quality control and efficient handling of sensitive information. | am dedicated to enhancing scanning processes and contributing positively t\n",
      "\n",
      "=== JD - SKILLS TEXT (first 300 chars) ===\n",
      "\n",
      "Required skills: Python, machine learning, data analysis, statistics, SQL,\n",
      "experience with Pandas, NumPy, and building ML models for classification and regression.\n",
      "\n",
      "\n",
      "=== JD - RESPONSIBILITIES TEXT (first 300 chars) ===\n",
      "\n",
      "Responsibilities: build and deploy machine learning models, analyze large datasets,\n",
      "create reports and dashboards, work with stakeholders to understand business problems,\n",
      "and improve existing data pipelines.\n",
      "\n",
      "\n",
      "=== SECTION-WISE SIMILARITY SCORES ===\n",
      "JD skills  vs  Resume skills       : -0.0911\n",
      "JD respons. vs  Resume experience  : 0.3046\n"
     ]
    }
   ],
   "source": [
    "from embedding import get_embedding, similarity\n",
    "from resume_sections import extract_resume_sections\n",
    "from file_io import extract_pdf_text\n",
    "\n",
    "# 1) Load full resume text again\n",
    "resume_path = \"sample_data/scanned_resume.pdf.pdf\"\n",
    "full_text = extract_pdf_text(resume_path)\n",
    "\n",
    "# 2) Extract sections from resume\n",
    "sections = extract_resume_sections(full_text)\n",
    "resume_skills_text = sections[\"skills\"]\n",
    "resume_exp_text = sections[\"experience\"]\n",
    "\n",
    "print(\"=== RESUME - SKILLS SECTION (first 300 chars) ===\")\n",
    "print(resume_skills_text[:300])\n",
    "\n",
    "print(\"\\n=== RESUME - EXPERIENCE SECTION (first 300 chars) ===\")\n",
    "print(resume_exp_text[:300])\n",
    "\n",
    "# 3) Define JD \"skills\" and \"responsibilities\" as separate strings\n",
    "jd_skills_text = \"\"\"\n",
    "Required skills: Python, machine learning, data analysis, statistics, SQL,\n",
    "experience with Pandas, NumPy, and building ML models for classification and regression.\n",
    "\"\"\"\n",
    "\n",
    "jd_responsibilities_text = \"\"\"\n",
    "Responsibilities: build and deploy machine learning models, analyze large datasets,\n",
    "create reports and dashboards, work with stakeholders to understand business problems,\n",
    "and improve existing data pipelines.\n",
    "\"\"\"\n",
    "\n",
    "print(\"\\n=== JD - SKILLS TEXT (first 300 chars) ===\")\n",
    "print(jd_skills_text[:300])\n",
    "\n",
    "print(\"\\n=== JD - RESPONSIBILITIES TEXT (first 300 chars) ===\")\n",
    "print(jd_responsibilities_text[:300])\n",
    "\n",
    "# 4) Compute embeddings for each section (only if text is non-empty)\n",
    "def safe_embedding(text: str):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    return get_embedding(text)\n",
    "\n",
    "resume_skills_emb = safe_embedding(resume_skills_text)\n",
    "resume_exp_emb = safe_embedding(resume_exp_text)\n",
    "jd_skills_emb = safe_embedding(jd_skills_text)\n",
    "jd_resp_emb = safe_embedding(jd_responsibilities_text)\n",
    "\n",
    "# 5) Compute section-wise similarities (with checks)\n",
    "print(\"\\n=== SECTION-WISE SIMILARITY SCORES ===\")\n",
    "\n",
    "if jd_skills_emb is not None and resume_skills_emb is not None:\n",
    "    score_skills = similarity(jd_skills_emb, resume_skills_emb)\n",
    "    print(f\"JD skills  vs  Resume skills       : {score_skills:.4f}\")\n",
    "else:\n",
    "    print(\"JD skills  vs  Resume skills       : cannot compute (missing text).\")\n",
    "\n",
    "if jd_resp_emb is not None and resume_exp_emb is not None:\n",
    "    score_exp = similarity(jd_resp_emb, resume_exp_emb)\n",
    "    print(f\"JD respons. vs  Resume experience  : {score_exp:.4f}\")\n",
    "else:\n",
    "    print(\"JD respons. vs  Resume experience  : cannot compute (missing text).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ed605d06-e838-47a2-be21-a956bed4592e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== RESUME RANKING FOR THIS JD ===\n",
      "\n",
      "1. sample_data/resume_pdf1.pdf.pdf\n",
      "   Raw score   : 0.5401\n",
      "   Human score : 54.0% - Weak match\n",
      "\n",
      "2. sample_data/scanned_resume.pdf.pdf\n",
      "   Raw score   : 0.0923\n",
      "   Human score : 9.2% - Poor match\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from file_io import extract_pdf_text\n",
    "from embedding import get_embedding, similarity\n",
    "\n",
    "# 1) Define your JD text (you can modify this anytime)\n",
    "jd_text = \"\"\"\n",
    "We are looking for a data scientist with strong skills in Python, machine learning,\n",
    "data analysis, and experience building classification and regression models.\n",
    "The candidate should know Pandas, NumPy, statistics, and SQL.\n",
    "\"\"\"\n",
    "\n",
    "# 2) List of resume PDF paths to compare\n",
    "#    TODO: Add more resume files here when you have them.\n",
    "resume_paths = [\n",
    "    \"sample_data/resume_pdf1.pdf.pdf\",\n",
    "    \"sample_data/scanned_resume.pdf.pdf\",\n",
    "    # \"sample_data/resume_ds2.pdf\",\n",
    "]\n",
    "\n",
    "# 3) Compute JD embedding once\n",
    "jd_emb = get_embedding(jd_text)\n",
    "\n",
    "def explain_score(score: float) -> str:\n",
    "    \"\"\"\n",
    "    Convert similarity score to human-readable percentage and label.\n",
    "    \"\"\"\n",
    "    percentage = score * 100\n",
    "\n",
    "    if score >= 0.75:\n",
    "        level = \"Excellent match\"\n",
    "    elif score >= 0.55:\n",
    "        level = \"Good match\"\n",
    "    elif score >= 0.35:\n",
    "        level = \"Weak match\"\n",
    "    else:\n",
    "        level = \"Poor match\"\n",
    "\n",
    "    return f\"{percentage:.1f}% - {level}\"\n",
    "\n",
    "# 4) For each resume, compute similarity vs JD\n",
    "results = []\n",
    "for path in resume_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"WARNING: File not found -> {path}\")\n",
    "        continue\n",
    "\n",
    "    text = extract_pdf_text(path)\n",
    "    emb = get_embedding(text)\n",
    "    score = similarity(jd_emb, emb)\n",
    "\n",
    "    results.append({\n",
    "        \"path\": path,\n",
    "        \"score\": score,\n",
    "        \"explanation\": explain_score(score),\n",
    "    })\n",
    "\n",
    "# 5) Sort results: highest similarity first\n",
    "results_sorted = sorted(results, key=lambda x: x[\"score\"], reverse=True)\n",
    "\n",
    "# 6) Print summary ranking\n",
    "print(\"=== RESUME RANKING FOR THIS JD ===\\n\")\n",
    "for rank, item in enumerate(results_sorted, start=1):\n",
    "    print(f\"{rank}. {item['path']}\")\n",
    "    print(f\"   Raw score   : {item['score']:.4f}\")\n",
    "    print(f\"   Human score : {item['explanation']}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372cbc45-1ff6-47c0-b514-d9b210146a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== DETAILED RESUME MATCH REPORT ===\n",
      "\n",
      "Resume: sample_data/resume_pdf1.pdf.pdf\n",
      "  Overall match     : 54.0% (Weak)\n",
      "  Skills match      : 48.0% (Weak)\n",
      "  Experience match  : 61.1% (Good)\n",
      "\n",
      "Resume: sample_data/scanned_resume.pdf.pdf\n",
      "  Overall match     : 9.2% (Poor)\n",
      "  Skills match      : -9.1% (Poor)\n",
      "  Experience match  : 30.5% (Poor)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from file_io import extract_pdf_text\n",
    "from resume_sections import extract_resume_sections\n",
    "from embedding import get_embedding, similarity\n",
    "\n",
    "# 1) Define JD texts (you can tweak these later)\n",
    "jd_full_text = \"\"\"\n",
    "We are looking for a data scientist with strong skills in Python, machine learning,\n",
    "data analysis, and experience building classification and regression models.\n",
    "The candidate should know Pandas, NumPy, statistics, and SQL.\n",
    "\"\"\"\n",
    "\n",
    "jd_skills_text = \"\"\"\n",
    "Required skills: Python, machine learning, data analysis, statistics, SQL,\n",
    "experience with Pandas, NumPy, and building ML models for classification and regression.\n",
    "\"\"\"\n",
    "\n",
    "jd_responsibilities_text = \"\"\"\n",
    "Responsibilities: build and deploy machine learning models, analyze large datasets,\n",
    "create reports and dashboards, work with stakeholders to understand business problems,\n",
    "and improve existing data pipelines.\n",
    "\"\"\"\n",
    "\n",
    "# 2) List of resume PDF paths to evaluate\n",
    "resume_paths = [\n",
    "   \"sample_data/resume_pdf1.pdf.pdf\",\n",
    "    \"sample_data/scanned_resume.pdf.pdf\",\n",
    "    # \"sample_data/resume_ds2.pdf\",\n",
    "]\n",
    "\n",
    "# 3) Helper to avoid errors when text is empty\n",
    "def safe_emb(text: str):\n",
    "    text = text.strip()\n",
    "    if not text:\n",
    "        return None\n",
    "    return get_embedding(text)\n",
    "\n",
    "# 4) Precompute JD embeddings (so we don't recompute for every resume)\n",
    "jd_full_emb = safe_emb(jd_full_text)\n",
    "jd_skills_emb = safe_emb(jd_skills_text)\n",
    "jd_resp_emb = safe_emb(jd_responsibilities_text)\n",
    "\n",
    "def explain_score(score: float) -> str:\n",
    "    if score is None:\n",
    "        return \"N/A\"\n",
    "    percentage = score * 100\n",
    "    if score >= 0.75:\n",
    "        level = \"Excellent\"\n",
    "    elif score >= 0.55:\n",
    "        level = \"Good\"\n",
    "    elif score >= 0.35:\n",
    "        level = \"Weak\"\n",
    "    else:\n",
    "        level = \"Poor\"\n",
    "    return f\"{percentage:.1f}% ({level})\"\n",
    "\n",
    "# 5) Score each resume\n",
    "report = []\n",
    "for path in resume_paths:\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"WARNING: file not found: {path}\")\n",
    "        continue\n",
    "\n",
    "    full_text = extract_pdf_text(path)\n",
    "    sections = extract_resume_sections(full_text)\n",
    "    resume_skills_text = sections[\"skills\"]\n",
    "    resume_exp_text = sections[\"experience\"]\n",
    "\n",
    "    # Compute embeddings\n",
    "    resume_full_emb = safe_emb(full_text)\n",
    "    resume_skills_emb = safe_emb(resume_skills_text)\n",
    "    resume_exp_emb = safe_emb(resume_exp_text)\n",
    "\n",
    "    # Compute scores (None if missing)\n",
    "    overall = similarity(jd_full_emb, resume_full_emb) if (jd_full_emb is not None and resume_full_emb is not None) else None\n",
    "    skills = similarity(jd_skills_emb, resume_skills_emb) if (jd_skills_emb is not None and resume_skills_emb is not None) else None\n",
    "    experience = similarity(jd_resp_emb, resume_exp_emb) if (jd_resp_emb is not None and resume_exp_emb is not None) else None\n",
    "\n",
    "    report.append({\n",
    "        \"path\": path,\n",
    "        \"overall\": overall,\n",
    "        \"skills\": skills,\n",
    "        \"experience\": experience,\n",
    "    })\n",
    "\n",
    "# 6) Sort by overall score (highest first), keeping None at the end\n",
    "report_sorted = sorted(\n",
    "    report,\n",
    "    key=lambda r: (r[\"overall\"] is None, -(r[\"overall\"] or 0.0))\n",
    ")\n",
    "\n",
    "# 7) Print a small report\n",
    "print(\"=== DETAILED RESUME MATCH REPORT ===\\n\")\n",
    "for item in report_sorted:\n",
    "     print(f\"Resume: {item['path']}\")\n",
    "     print(f\"  Overall match     : {explain_score(item['overall'])}\")\n",
    "     print(f\"  Skills match      : {explain_score(item['skills'])}\")\n",
    "     print(f\"  Experience match  : {explain_score(item['experience'])}\")\n",
    "     print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dc0e3ce-8114-43f8-bb7f-4bead5598036",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Resume venv",
   "language": "python",
   "name": "resume-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
